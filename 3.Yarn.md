# YARN:(Yet Another Resource Negotiator)

YARN can be thought of as analogous of an operating system for a cluster.
A cluster is a set of loosely or tightly connected computers that work together to be viewed as a single system. 
The cluster represents the collection of resources, such as compute, memory, disk-space, and network bandwidth, 
that YARN must arbitrate among jobs that run on the cluster. Similar to how an operating system presides over the machineâ€™s resources and distributes 
them among competing processes, YARN allocates cluster resources among competing jobs. 
The following picture shows where YARN sits in the Hadoop stack:

![image](https://user-images.githubusercontent.com/33947539/183062739-93019264-ea7d-40aa-b6c6-6962939af295.png)

Apart from YARN there are other cluster managers like instance Apache Mesos, Googleâ€™s closed-source Borg Scheduler.

ðŸ‘‰ ***It has two major responsibilities:***

Management of cluster resources such as compute, network, and memory
Scheduling and monitoring of jobs
YARN achieves these goals through two long-running daemons:

- **Resource Manager**
- **Node Manager**

![image](https://user-images.githubusercontent.com/33947539/183063201-b1139722-7da1-43fc-9fa5-afc639bd90ed.png)

The two components work in a master-slave relationship, where the **Resource Manager(RM)** is the master and the **Node Managers** the slave. A single Resource Manager runs in the cluster with one Node Manager per machine. 
Together, these two components make up the data-computation framework. Letâ€™s discuss the resource manager first.

### Resource Manager
Applications Manager: is responsible for accepting job submissions and starting a container for an entity called the ApplicationMaster. It also restarts the ApplicationMaster container if the container fails. Weâ€™ll discuss the details around the ApplicationMaster shortly.

Scheduler: The scheduler is responsible for allocating resources such as disk, CPU, and network running applications, subject to restrictions imposed by queues and capacity. The scheduler does not monitor the applications nor does it initiate restarts on application or hardware failures.

>Note the word container here. Donâ€™t confuse it with an application container such as Docker. In the context of YARN, container is an abstract notion that represents resources such as disk, memory, CPU, network, and others. A container grants rights to an application to use a specific amount of resources on a specific host.

In Unix, container is a process and in Linux a cgroup. Map and reduce tasks run inside a container. A single machine in the cluster can have multiple containers.

The astute reader would realize that the Resource Manager acts as a single point of failure. If the machine hosting the RM goes down, no jobs can get scheduled. To mitigate this shortcoming high availability for YARN was introduced in Hadoop 2.4. A pair of Resource Managers runs in Active/Standby configuration to achieve high availability. If the active Resource Manager dies, then the standby one becomes the active and the cluster continues to function correctly. The transition from standby to active mode can be done manually by an administrator or automatically. For automatic transition, Zookeeper is required for election.

![image](https://user-images.githubusercontent.com/33947539/183063888-abba6ad8-e549-4263-8ea3-4cd072c0b723.png)

### Node Manager
The NodeManager is an agent that runs on every machine in the cluster. It is responsible for launching containers on that machine and managing the use of resources by the containers. It reports the usage back to the Scheduler component of the Resource Manager.

# Workflow
The first step of running a YARN application involves requesting the RM (resource manager) to create an Application Master(AM) process. 
A client submits a job and the RM finds a Node Manager that can launch a container to host the AM process. 
The AM process represents the client job/application. 
It can either run the job itself and return or request for additional resources from the RM. In the latter, the RM has Node Managers on other machines launch containers on behalf of the AM process to run the distributed computation. Nodes chosen for new container allocations allow the computation to execute as close as possible to the input data also known as data locality. Ideally, the container is allocated on a node hosting a replica of the data block. 
The next preference is a node in the same rack as the input data block, and lastly any available node in the cluster.

